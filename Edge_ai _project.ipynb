{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8145bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from serial.tools import list_ports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import serial\n",
    "from IPython.display import Audio, display, HTML\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa73444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aec2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COM3 - USB Serial Device (COM3)\n"
     ]
    }
   ],
   "source": [
    "ports = list_ports.comports()\n",
    "for port in ports:\n",
    "    print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b6d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import serial.tools.list_ports\n",
    "\n",
    "# ports = serial.tools.list_ports.comports()\n",
    "# for port in ports:\n",
    "#     print(port.device, \"-\", port.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32513a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'x' to Activate Arduino: x\n",
      "Success Activated!\n",
      "Open the Serial Monitor to check if it is working, then close it.\n"
     ]
    }
   ],
   "source": [
    "SERIAL_PORT = \"COM3\"\n",
    "BAUD_RATE = 38400\n",
    "\n",
    "def arduino_activate():\n",
    "    try:\n",
    "        arduino = serial.Serial(SERIAL_PORT, BAUD_RATE, timeout=1)\n",
    "        time.sleep(2)  # Allow time for connection to establish\n",
    "    except serial.SerialException as e:\n",
    "        print(f\"Failed to connect to serial port {SERIAL_PORT}: \\n{e}\")\n",
    "        return None\n",
    "\n",
    "    command = input(\"Type 'x' to Activate Arduino: \")\n",
    "    if command.lower() == 'x':\n",
    "        arduino.write(command.encode('utf-8'))\n",
    "        print(\"Success Activated!\")\n",
    "        print(\"Open the Serial Monitor to check if it is working, then close it.\")\n",
    "    return arduino\n",
    "\n",
    "\n",
    "arduino = arduino_activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c037ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating: Please Speak to the Microphone\n",
      "Normalized signal from the range (-1998.0, 1704.0) to (-1, 1)\n"
     ]
    }
   ],
   "source": [
    "def arduino_read(buffer, buffer_size, overlapping, norm=(None, None), max_attempts=10):\n",
    "    buffer = np.roll(buffer, overlapping)\n",
    "    num_data = buffer_size - overlapping\n",
    "    for i in range(num_data):\n",
    "        decoded_data = ''\n",
    "        attempts = 0\n",
    "        while decoded_data == '':\n",
    "            arduino_data = arduino.readline()\n",
    "            decoded_data = arduino_data[:len(arduino_data)].decode(\"utf-8\").strip('\\r\\n')\n",
    "            attempts += 1\n",
    "            if attempts >= max_attempts:\n",
    "                print('Fail to Retrieve Data...')\n",
    "                break\n",
    "        if norm[0] is not None:\n",
    "            decoded_data = normalize(int(decoded_data), 1, -1, norm[0], norm[1])\n",
    "        try:\n",
    "            buffer[i+overlapping] = decoded_data\n",
    "        except ValueError:\n",
    "            print(\"Open the Serial Monitor to check if it is working. If it's not, press the reset button and rerun 'arduino_activate' function again.\")\n",
    "    return buffer\n",
    "\n",
    "def normalize(array, new_max, new_min, old_max, old_min):\n",
    "    array = (((array - old_min) * (new_max - new_min)) / (old_max - old_min)) + new_min\n",
    "    return array.astype(np.float16)\n",
    "\n",
    "\n",
    "# Calibrate the microphone\n",
    "print(\"Calibrating: Please Speak to the Microphone\")\n",
    "time.sleep(1)\n",
    "tuning_data = arduino_read(np.zeros(48000), 48000, 0)\n",
    "TUNING_MAX, TUNING_MIN = (max(tuning_data), min(tuning_data))\n",
    "print(f'Normalized signal from the range ({TUNING_MIN}, {TUNING_MAX}) to (-1, 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3097812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ccc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e3a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e9f3a",
   "metadata": {},
   "source": [
    "### Correctly running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119fa1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33832064 36.36630401 13.53465915 25.33108592]\n",
      "Emotion: neutral\n",
      "\n",
      "[2.64457834e-04 7.27865822e-01 3.56483902e-01 4.98209059e+01]\n",
      "Emotion: angry\n",
      "\n",
      "[9.84158687e-05 3.19786195e-01 1.54644030e-01 4.99224484e+01]\n",
      "Emotion: angry\n",
      "\n",
      "[7.57678691e-03 9.16127209e+00 3.74204740e+00 4.76265281e+01]\n",
      "Emotion: angry\n",
      "\n",
      "[6.62807288e-05 2.69259996e-01 5.49055450e-01 4.98831809e+01]\n",
      "Emotion: angry\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time for 5 rounds: 33.853081941604614 seconds\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 24000\n",
    "OVERLAPPED = 512\n",
    "\n",
    "NUM_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "EMOTIONS = ['sad','neutral', 'happy', 'angry']\n",
    "COMMANDS = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# Manually Calibrate Sensitivity of Recognition\n",
    "RECOG_MASK = np.array([1, 300, 400, 50])\n",
    "\n",
    "\n",
    "async def tflite_process_data():\n",
    "    data = np.zeros(BUFFER_SIZE)\n",
    "    data = arduino_read(data, BUFFER_SIZE, OVERLAPPED, norm=(TUNING_MAX, TUNING_MIN))\n",
    "#     data /= np.iinfo(np.int16).max\n",
    "#     print(data.shape)\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    features = np.array([mfcc.T], dtype=np.float32)\n",
    "\n",
    "    # Model Input Shape = (None, None, 13)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], features)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]\n",
    "    result = np.multiply(prediction, RECOG_MASK)\n",
    "    emotion = EMOTIONS[np.argmax(result)]\n",
    "    command = COMMANDS[np.argmax(result)]\n",
    "    arduino.write(command.encode('utf-8'))\n",
    "    print(result)\n",
    "    print(f'Emotion: {emotion}\\n')\n",
    "    \n",
    "async def tflite_run(rounds=5):\n",
    "    tasks = []\n",
    "    start_time = time.time()\n",
    "    for turn in range(rounds):\n",
    "        task = asyncio.create_task(tflite_process_data())\n",
    "        tasks.append(task)\n",
    "        time.sleep(0.6)\n",
    "    await asyncio.gather(*tasks)\n",
    "    display(HTML(\"<hr>\"))\n",
    "    print(f\"Inference time for {turn+1} rounds: {time.time() - start_time} seconds\") \n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"SER.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "await tflite_run(rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd21af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp39-cp39-win_amd64.whl (164 kB)\n",
      "     -------------------------------------- 164.1/164.1 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\siddh\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e85888",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d15a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import librosa\n",
    "import time\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a45d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Constants\n",
    "# BUFFER_SIZE = 24000\n",
    "# OVERLAPPED = 512\n",
    "\n",
    "# NUM_MFCC = 13\n",
    "# N_FFT = 2048\n",
    "# HOP_LENGTH = 512\n",
    "# SAMPLE_RATE = 16000\n",
    "\n",
    "# EMOTIONS = ['neutral', 'happy', 'surprise', 'unpleasant']\n",
    "# COMMANDS = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# # Manually Calibrate Sensitivity of Recognition\n",
    "# RECOG_MASK = np.array([1, 30000, 40000, 500])\n",
    "\n",
    "# # Initialize speech recognizer\n",
    "# recognizer = sr.Recognizer()\n",
    "\n",
    "# async def tflite_process_data():\n",
    "#     data = np.zeros(BUFFER_SIZE)\n",
    "#     data = arduino_read(data, BUFFER_SIZE, OVERLAPPED, norm=(TUNING_MAX, TUNING_MIN))\n",
    "    \n",
    "#     # Speech recognition\n",
    "#     with sr.AudioFile(data) as source:\n",
    "#         audio_data = recognizer.record(source)\n",
    "#         try:\n",
    "#             text = recognizer.recognize_google(audio_data)\n",
    "#             print(f\"Recognized Text: {text}\")\n",
    "#         except sr.UnknownValueError:\n",
    "#             print(\"Google Speech Recognition could not understand audio\")\n",
    "#         except sr.RequestError as e:\n",
    "#             print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "    \n",
    "#     # MFCC feature extraction\n",
    "#     mfcc = librosa.feature.mfcc(y=data, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "#     features = np.array([mfcc.T], dtype=np.float32)\n",
    "\n",
    "#     # Model Inference\n",
    "#     interpreter.set_tensor(interpreter.get_input_details()[0]['index'], features)\n",
    "#     interpreter.invoke()\n",
    "#     prediction = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]\n",
    "#     result = np.multiply(prediction, RECOG_MASK)\n",
    "#     emotion = EMOTIONS[np.argmax(result)]\n",
    "#     command = COMMANDS[np.argmax(result)]\n",
    "#     arduino.write(command.encode('utf-8'))\n",
    "#     print(f\"Emotion Detected: {emotion}\")\n",
    "    \n",
    "# async def tflite_run(rounds=10):\n",
    "#     tasks = []\n",
    "#     start_time = time.time()\n",
    "#     for turn in range(rounds):\n",
    "#         task = asyncio.create_task(tflite_process_data())\n",
    "#         tasks.append(task)\n",
    "#         time.sleep(0.6)\n",
    "#     await asyncio.gather(*tasks)\n",
    "#     display(HTML(\"<hr>\"))\n",
    "#     print(f\"Inference time for {turn+1} rounds: {time.time() - start_time} seconds\") \n",
    "\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"SER_quant.tflite\")\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# await tflite_run(rounds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6235f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b8cd5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording stopped.\n",
      "Recognized Text: today I am happy\n",
      "(24000,)\n",
      "[3.64670134e-03 4.47887823e+03 3.24245572e+04 1.82217360e+01]\n",
      "Emotion Detected: surprise\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import asyncio\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Constants\n",
    "BUFFER_SIZE = 24000\n",
    "OVERLAPPED = 512\n",
    "NUM_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAMPLE_RATE = 9600\n",
    "EMOTIONS = ['sad','neutral', 'happy', 'angry']\n",
    "COMMANDS = ['a', 'b', 'c', 'd']\n",
    "RECOG_MASK = np.array([1, 30000, 40000, 500])\n",
    "\n",
    "# Initialize TensorFlow Lite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"SER_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Initialize the recognizer and microphone\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "async def record_and_process_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=SAMPLE_RATE, input=True, frames_per_buffer=BUFFER_SIZE)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(SAMPLE_RATE / BUFFER_SIZE*4.5)):  # Record for 5 seconds\n",
    "        data = stream.read(BUFFER_SIZE)\n",
    "#         data = np.zeros(BUFFER_SIZE)\n",
    "#         data = arduino_read(data, BUFFER_SIZE, OVERLAPPED, norm=(TUNING_MAX, TUNING_MIN))\n",
    "    \n",
    "        frames.append(data)\n",
    "    print(\"Recording stopped.\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    # Convert frames to byte array\n",
    "    audio_data = b''.join(frames)\n",
    "    source = sr.AudioData(audio_data, SAMPLE_RATE, 2)  # Corrected this line\n",
    "    \n",
    "    data = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32)\n",
    "    # Normalize the audio data to range [-1, 1] (as expected by librosa)\n",
    "#     data /= np.iinfo(np.int16).max\n",
    "    # Use the audio data for speech recognition\n",
    "#     with sr.AudioData(audio_data, SAMPLE_RATE, 2) as source:\n",
    "    \n",
    "    try:\n",
    "        text = recognizer.recognize_google(source)\n",
    "        print(f\"Recognized Text: {text}\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "    \n",
    "    # Convert byte data to NumPy array for MFCC\n",
    "#     data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "    print(data.shape)\n",
    "#     print(data[0])\n",
    "#     print(data[24000])\n",
    "#     print(data[48000])\n",
    "\n",
    "    # MFCC feature extraction\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    features = np.array([mfcc.T], dtype=np.float32)\n",
    "\n",
    "    # Model Inference\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], features)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]\n",
    "    result = np.multiply(prediction, RECOG_MASK)\n",
    "    emotion = EMOTIONS[np.argmax(result)]\n",
    "    command = COMMANDS[np.argmax(result)]\n",
    "    arduino.write(command.encode('utf-8'))\n",
    "    print(result)\n",
    "    print(f\"Emotion Detected: {emotion}\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "async def tflite_run(rounds=1):\n",
    "    tasks = []\n",
    "    for _ in range(rounds):\n",
    "        task = asyncio.create_task(record_and_process_audio())\n",
    "        tasks.append(task)\n",
    "        await asyncio.sleep(0.6)  # Wait before starting next recording\n",
    "    await asyncio.gather(*tasks)\n",
    "    display(HTML(\"<hr>\"))\n",
    "    print(\"Inference completed.\")\n",
    "\n",
    "# Run the asynchronous inference loop\n",
    "await tflite_run(rounds=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "726970cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape: [ 1 47 13]\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "expected_shape = input_details[0]['shape']\n",
    "print(\"Expected shape:\", expected_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "77d9f8f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21192\\260430750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# Run the asynchronous inference loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[1;32mawait\u001b[0m \u001b[0mtflite_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21192\\260430750.py\u001b[0m in \u001b[0;36mtflite_run\u001b[1;34m(rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Wait before starting next recording\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<hr>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inference completed.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21192\\260430750.py\u001b[0m in \u001b[0;36mtflite_process_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Convert the data to a format suitable for speech recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#     recognizer = sr.Recognizer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mrecognized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognize_speech_from_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#     with sr.AudioFile(data) as source:  # Assuming data can be treated like an audio file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21192\\921777279.py\u001b[0m in \u001b[0;36mrecognize_speech_from_array\u001b[1;34m(data, sample_rate)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mrecognizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maudio_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy_array_to_audio_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mAudioData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Use recognizer to convert speech to text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import asyncio\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Constants defined as before\n",
    "\n",
    "BUFFER_SIZE = 24000\n",
    "OVERLAPPED = 512\n",
    "\n",
    "NUM_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "EMOTIONS = ['neutral', 'happy', 'surprise', 'unpleasant']\n",
    "COMMANDS = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# Manually Calibrate Sensitivity of Recognition\n",
    "RECOG_MASK = np.array([1, 30000, 40000, 500])\n",
    "\n",
    "\n",
    "\n",
    "async def tflite_process_data():\n",
    "    data = np.zeros(BUFFER_SIZE)\n",
    "    data = arduino_read(data, BUFFER_SIZE, OVERLAPPED, norm=(TUNING_MAX, TUNING_MIN))\n",
    "#     data = data.astype(np.float32) / np.iinfo(np.int16).max  # Normalize audio data to [-1, 1]\n",
    "\n",
    "    # Convert the data to a format suitable for speech recognition\n",
    "#     recognizer = sr.Recognizer()\n",
    "#     recognized_text = recognize_speech_from_array(data, SAMPLE_RATE)\n",
    "    \n",
    "#     with sr.AudioFile(data) as source:  # Assuming data can be treated like an audio file\n",
    "#         audio = recognizer.record(source)\n",
    "\n",
    "#     try:\n",
    "#         # Recognize speech using Google's speech recognition\n",
    "#         recognized_text = recognizer.recognize_google(audio)\n",
    "#         print(f\"Recognized sentence: {recognized_text}\")\n",
    "#     except sr.UnknownValueError:\n",
    "#         print(\"Google Speech Recognition could not understand audio\")\n",
    "#         recognized_text = \"\"\n",
    "#     except sr.RequestError as e:\n",
    "#         print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "#         recognized_text = \"\"\n",
    "\n",
    "    # Proceed with emotion detection if speech was recognized\n",
    "#     if recognized_text:\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    features = np.array([mfcc.T], dtype=np.float32)\n",
    "\n",
    "    # Model Inference\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], features)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]\n",
    "    result = np.multiply(prediction, RECOG_MASK)\n",
    "    emotion = EMOTIONS[np.argmax(result)]\n",
    "    command = COMMANDS[np.argmax(result)]\n",
    "    arduino.write(command.encode('utf-8'))\n",
    "    print(result)\n",
    "    print(f'Emotion: {emotion}\\n')\n",
    "    # Send command to Arduino\n",
    "#     arduino.write(command.encode('utf-8'))\n",
    "#     else:\n",
    "#         print(\"No speech detected, skipping emotion detection.\")\n",
    "\n",
    "# Rest of your code for running and initializing remains unchanged\n",
    "    \n",
    "    \n",
    "async def tflite_run(rounds=2):\n",
    "    tasks = []\n",
    "    for _ in range(rounds):\n",
    "        task = asyncio.create_task(tflite_process_data())\n",
    "        tasks.append(task)\n",
    "        await asyncio.sleep(0.6)  # Wait before starting next recording\n",
    "    await asyncio.gather(*tasks)\n",
    "    display(HTML(\"<hr>\"))\n",
    "    print(\"Inference completed.\")\n",
    "\n",
    "# Run the asynchronous inference loop\n",
    "await tflite_run(rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b27fa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import soundfile as sf\n",
    "from speech_recognition import Recognizer, AudioData\n",
    "\n",
    "def numpy_array_to_audio_file(data, sample_rate=16000):\n",
    "    \"\"\" Convert a NumPy array to an audio file in memory. \"\"\"\n",
    "    virtual_file = io.BytesIO()\n",
    "    sf.write(virtual_file, data, samplerate=sample_rate, format='WAV', subtype='PCM_16')\n",
    "    virtual_file.seek(0)  # Important: return to the start of the file before reading\n",
    "    return virtual_file\n",
    "\n",
    "def recognize_speech_from_array(data, sample_rate=16000):\n",
    "    \"\"\" Recognize speech using the speech_recognition library. \"\"\"\n",
    "    recognizer = Recognizer()\n",
    "    audio_file = numpy_array_to_audio_file(data, sample_rate)\n",
    "    with AudioData(audio_file, sample_rate, 2) as source:\n",
    "        # Use recognizer to convert speech to text\n",
    "        try:\n",
    "            text = recognizer.recognize_google(source)\n",
    "            print(\"Recognized text:\", text)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(\"Error recognizing speech:\", str(e))\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "857fa0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording ended.\n",
      "Error recognizing speech: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import soundfile as sf\n",
    "from speech_recognition import Recognizer, AudioData\n",
    "import serial  # For serial communication with Arduino\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize TensorFlow Lite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=\"SER_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Initialize the recognizer and microphone\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def numpy_array_to_audio_file(data, sample_rate=16000):\n",
    "    \"\"\"Convert a NumPy array to an audio file in memory.\"\"\"\n",
    "    virtual_file = io.BytesIO()\n",
    "    sf.write(virtual_file, data, samplerate=sample_rate, format='WAV', subtype='PCM_16')\n",
    "    virtual_file.seek(0)  # Important: return to the start of the file before reading\n",
    "    return virtual_file\n",
    "\n",
    "def recognize_speech_from_array(data, sample_rate=16000):\n",
    "    \"\"\"Recognize speech using the speech_recognition library.\"\"\"\n",
    "    recognizer = Recognizer()\n",
    "    audio_file = numpy_array_to_audio_file(data, sample_rate)\n",
    "    \n",
    "    source = sr.AudioData(audio_file.read(), sample_rate, 2)\n",
    "#     with AudioData(audio_file, sample_rate, 2) as source:\n",
    "    try:\n",
    "        text = recognizer.recognize_google(source)\n",
    "        print(\"Recognized text:\", text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error recognizing speech:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Example setup for reading and processing\n",
    "# arduino = serial.Serial('COM_PORT', 9600, timeout=1)  # Update 'COM_PORT' with your actual COM port\n",
    "BUFFER_SIZE = 24000\n",
    "OVERLAPPED = 512\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "data = np.zeros(BUFFER_SIZE)\n",
    "print(\"Recording started...\")\n",
    "data = arduino_read(data, BUFFER_SIZE, OVERLAPPED, norm=(TUNING_MAX, TUNING_MIN))\n",
    "if data is not None:\n",
    "    print(\"Recording ended.\")\n",
    "    recognized_text = recognize_speech_from_array(data, SAMPLE_RATE)\n",
    "else:\n",
    "    print(\"Recording failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e679706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_15248\\1862110629.py:64: RuntimeWarning: coroutine 'record_and_process_audio' was never awaited\n",
      "  recognized_text = record_and_process_audio()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85749378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
