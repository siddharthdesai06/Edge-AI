{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b63da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965f3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba16728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a5ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install pyrealsense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d55828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768b4d9",
   "metadata": {},
   "source": [
    "## 1) Checking the working of intel real sense camera module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c846d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "        \n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        # Apply colormap on depth image (optional)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # Stack both images horizontally\n",
    "        images = np.hstack((color_image, depth_colormap))\n",
    "        \n",
    "        # Show images\n",
    "        cv2.imshow('RealSense Camera', images)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119d7ce",
   "metadata": {},
   "source": [
    "## 2) Creating Image model for video emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972ede71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output =  torch.Size([1, 7])\n",
      "No of Parameters of the BatchNorm-CNN Model = 1642935\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Face_Emotion_CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Face_Emotion_CNN, self).__init__()\n",
    "    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "    self.cnn2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "    self.cnn3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "    self.cnn4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "    self.cnn5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "    self.cnn6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "    self.cnn7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.pool1 = nn.MaxPool2d(2, 1)\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.cnn1_bn = nn.BatchNorm2d(8)\n",
    "    self.cnn2_bn = nn.BatchNorm2d(16)\n",
    "    self.cnn3_bn = nn.BatchNorm2d(32)\n",
    "    self.cnn4_bn = nn.BatchNorm2d(64)\n",
    "    self.cnn5_bn = nn.BatchNorm2d(128)\n",
    "    self.cnn6_bn = nn.BatchNorm2d(256)\n",
    "    self.cnn7_bn = nn.BatchNorm2d(256)\n",
    "    self.fc1 = nn.Linear(1024, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 7)\n",
    "    self.dropout = nn.Dropout(0.3)\n",
    "    self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.pool1(self.cnn1_bn(self.cnn1(x))))\n",
    "    x = self.relu(self.pool1(self.cnn2_bn(self.dropout(self.cnn2(x)))))\n",
    "    x = self.relu(self.pool1(self.cnn3_bn(self.cnn3(x))))\n",
    "    x = self.relu(self.pool1(self.cnn4_bn(self.dropout(self.cnn4(x)))))\n",
    "    x = self.relu(self.pool2(self.cnn5_bn(self.cnn5(x))))\n",
    "    x = self.relu(self.pool2(self.cnn6_bn(self.dropout(self.cnn6(x)))))\n",
    "    x = self.relu(self.pool2(self.cnn7_bn(self.dropout(self.cnn7(x)))))\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    \n",
    "    x = self.relu(self.dropout(self.fc1(x)))\n",
    "    x = self.relu(self.dropout(self.fc2(x)))\n",
    "    x = self.log_softmax(self.fc3(x))\n",
    "    return x\n",
    "\n",
    "  def count_parameters(self):\n",
    "    return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bn_model = Face_Emotion_CNN()\n",
    "    x = torch.randn(1,1,48,48)\n",
    "    print('Shape of output = ',bn_model(x).shape)\n",
    "    print('No of Parameters of the BatchNorm-CNN Model =',bn_model.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bd9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "# from model import *\n",
    "\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "def FER_live_cam():\n",
    "\n",
    "    model = load_trained_model('FER_trained_model.pt')\n",
    "    \n",
    "    emotion_dict = {0: 'neutral', 1: 'happiness'}\n",
    "\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "            resize_frame = cv2.resize(gray[y:y + h, x:x + w], (48, 48))\n",
    "            X = resize_frame/256\n",
    "            X = Image.fromarray((X))\n",
    "            X = val_transform(X).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                log_ps = model.cpu()(X)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                pred = emotion_dict[int(top_class.numpy())]\n",
    "            cv2.putText(frame, pred, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1)\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    FER_live_cam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36fe59",
   "metadata": {},
   "source": [
    "## Quantizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7bdca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 6.29 MB\n",
      "Quantized model size: 4.41 MB\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "def FER_live_cam():\n",
    "\n",
    "    model = load_trained_model('FER_trained_model.pt')\n",
    "    \n",
    "    emotion_dict = {0: 'neutral', 1: 'happiness'}\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "            resize_frame = cv2.resize(gray[y:y + h, x:x + w], (48, 48))\n",
    "            X = resize_frame/256\n",
    "            X = Image.fromarray((X))\n",
    "            X = val_transform(X).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                log_ps = model.cpu()(X)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                pred = emotion_dict[int(top_class.numpy())]\n",
    "            cv2.putText(frame, pred, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1)\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the original model\n",
    "    torch.save(model.state_dict(), 'original_model.pth')\n",
    "\n",
    "    # Quantize the model dynamically\n",
    "    quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "    # Save the quantized model\n",
    "    torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
    "\n",
    "    # Print the file sizes\n",
    "    original_size = os.path.getsize('original_model.pth')/(1024*1024)\n",
    "    quantized_size = os.path.getsize('quantized_model.pth')/(1024*1024)\n",
    "    print(\"Original model size:\", round(original_size, 2), \"MB\")\n",
    "    print(\"Quantized model size:\", round(quantized_size, 2), \"MB\")\n",
    "if __name__ == \"__main__\":\n",
    "    FER_live_cam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c09e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdce59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84111091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543d7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24e1ac20",
   "metadata": {},
   "source": [
    "# Transfer Learning on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1b2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Face_Emotion_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Face_Emotion_CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)  # Update input channels to 3\n",
    "        self.cnn1_bn = nn.BatchNorm2d(8)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.cnn2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.cnn2_bn = nn.BatchNorm2d(16)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.cnn3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.cnn3_bn = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 128)\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_bn = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 7)  # Output layer with 7 units for 7 classes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool1(self.cnn1_bn(self.cnn1(x))))\n",
    "        x = self.relu(self.pool1(self.cnn2_bn(self.dropout(self.cnn2(x)))))\n",
    "        x = self.relu(self.pool1(self.cnn3_bn(self.cnn3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda7af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7613\n",
      "Epoch [2/20], Loss: 0.7313\n",
      "Epoch [3/20], Loss: 0.7106\n",
      "Epoch [4/20], Loss: 0.6982\n",
      "Epoch [5/20], Loss: 0.7145\n",
      "Epoch [6/20], Loss: 0.6845\n",
      "Epoch [7/20], Loss: 0.6901\n",
      "Epoch [8/20], Loss: 0.6936\n",
      "Epoch [9/20], Loss: 0.6952\n",
      "Epoch [10/20], Loss: 0.7116\n",
      "Epoch [11/20], Loss: 0.7086\n",
      "Epoch [12/20], Loss: 0.7120\n",
      "Epoch [13/20], Loss: 0.7093\n",
      "Epoch [14/20], Loss: 0.7016\n",
      "Epoch [15/20], Loss: 0.7066\n",
      "Epoch [16/20], Loss: 0.7073\n",
      "Epoch [17/20], Loss: 0.7003\n",
      "Epoch [18/20], Loss: 0.6880\n",
      "Epoch [19/20], Loss: 0.7081\n",
      "Epoch [20/20], Loss: 0.6985\n",
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "# from model import Face_Emotion_CNN\n",
    "\n",
    "# Define data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more data augmentation transformations if needed\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Define dataset and dataloader\n",
    "train_dataset = ImageFolder(root=r\"Path_to_dataset\", transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "model = Face_Emotion_CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'FER_trained_model_new.pt')\n",
    "\n",
    "print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ef00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "# from model import Face_Emotion_CNN  # Import your model here\n",
    "\n",
    "# Load the trained model\n",
    "model = Face_Emotion_CNN()\n",
    "model.load_state_dict(torch.load('FER_trained_model_new.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation for input images\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_dict = {0: 'happy', 1: 'neutral'}\n",
    "\n",
    "# Open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    gray = frame\n",
    "    # Detect faces in the frame\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_roi = gray[y:y + h, x:x + w]\n",
    "\n",
    "        # Resize the face region to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (48, 48))\n",
    "\n",
    "        # Normalize and transform the face image\n",
    "        normalized_face = resized_face / 255.0  # Normalize to [0,1]\n",
    "        transformed_face = val_transform(normalized_face)\n",
    "\n",
    "        # Convert the input tensor to the same data type as the bias parameter\n",
    "        input_tensor = transformed_face.float()\n",
    "\n",
    "        # Add batch dimension and convert to tensor\n",
    "        input_tensor = torch.unsqueeze(input_tensor, dim=0)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model(input_tensor)\n",
    "\n",
    "            # Get the predicted emotion label\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            emotion_label = emotion_dict[predicted.item()]\n",
    "\n",
    "        # Draw the bounding box and emotion label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ebcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23f718d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0baf83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define the model architecture\n",
    "class FERPlus(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(FERPlus, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = FERPlus(num_classes=2)  # 8 classes for FERPlus dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd219ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FERPlus(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55ef9908",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No device connected",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19044\\459404501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m640\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbgr8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Define emotion labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No device connected"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Load the pre-trained ResNet50 model for emotion detection\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(2048, 7)  # 7 classes for emotions\n",
    "model.eval()\n",
    "\n",
    "# Load the RealSense camera\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "# Define emotion labels\n",
    "# emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotion_labels = ['Happy', 'Neutral']\n",
    "\n",
    "# Function to preprocess image for ResNet50\n",
    "def preprocess_image(image):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.fromarray(image)\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a color frame\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        \n",
    "        if not color_frame:\n",
    "            continue\n",
    "        \n",
    "        # Convert color image to numpy array\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Use Haar cascade to detect faces\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Iterate through faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract face region\n",
    "            face = color_image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Preprocess image for ResNet50\n",
    "            face_tensor = preprocess_image(face)\n",
    "            \n",
    "            # Perform emotion detection\n",
    "            with torch.no_grad():\n",
    "                outputs = model(face_tensor)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                emotion = emotion_labels[predicted.item()]\n",
    "            \n",
    "            # Draw rectangle around the face\n",
    "            cv2.rectangle(color_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "            # Put emotion label on the frame\n",
    "            cv2.putText(color_image, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "        \n",
    "        # Show the frame with emotions\n",
    "        cv2.imshow('Emotion Detection', color_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "q\n",
    "            \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e2721",
   "metadata": {},
   "source": [
    "## Creating Model for audio to emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a59440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Recording...\n",
      "Text: it was a good day today\n",
      "HAPPY\n",
      "Predicted Emotion: HAPPY\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import speech_recognition as sr\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pygame\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to start recording\n",
    "def start_recording():\n",
    "    # Record audio from the microphone for 10 seconds\n",
    "    print(\"Recording...\")\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)  # Adjust for ambient noise\n",
    "        audio_data = recognizer.record(source, duration=2)\n",
    "\n",
    "    # Recognize speech using Google Speech Recognition\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Text:\", text)\n",
    "        #emo = predict_emotion(text)\n",
    "        predicted_label = predict_emotion(text)\n",
    "        print(predicted_label)\n",
    "        recorded_text = f\"Recorded Text: {text}\"\n",
    "        predicted_text = f\"Predicted Emotion: {predicted_label}\"\n",
    "        print(predicted_text)\n",
    "        draw_text(recorded_text, predicted_label)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio.\")\n",
    "        draw_text(\"Google Speech Recognition could not understand the audio.\", \"\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "        draw_text(\"Could not request results from Google Speech Recognition service.\", \"\")\n",
    "\n",
    "# Function to predict emotion from text\n",
    "def predict_emotion(text):\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    # Map predicted label to emotion\n",
    "    emotion_mapping = {\n",
    "        0: \"SAD\",\n",
    "        1: \"SAD\",\n",
    "        2: \"Neutral\",\n",
    "        3: \"HAPPY\",\n",
    "        4: \"Excited\"\n",
    "    }\n",
    "\n",
    "    # Return predicted emotion\n",
    "    return emotion_mapping.get(predicted_label, \"Unknown\")\n",
    "\n",
    "# Function to draw text on the screen\n",
    "def draw_text(recorded_text, predicted_text):\n",
    "    screen.fill((255, 255, 255))\n",
    "    font = pygame.font.Font(None, 30)\n",
    "    text_surface1 = font.render(recorded_text, True, (0, 0, 0))\n",
    "    text_surface2 = font.render(predicted_text, True, (0, 0, 0))\n",
    "    screen.blit(text_surface1, (50, 50))\n",
    "    screen.blit(text_surface2, (50, 100))\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Set up the screen\n",
    "WIDTH, HEIGHT = 600, 200\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Emotion Recognition\")\n",
    "\n",
    "# Set up the button\n",
    "button_font = pygame.font.Font(None, 36)\n",
    "button_text = button_font.render(\"Start Recording\", True, (0, 0, 0))\n",
    "button_rect = button_text.get_rect(center=(WIDTH // 2, HEIGHT // 2))\n",
    "\n",
    "# Main loop\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            if button_rect.collidepoint(event.pos):\n",
    "                start_recording()\n",
    "\n",
    "    # Clear the screen\n",
    "    screen.fill((255, 255, 255))\n",
    "\n",
    "    # Draw the button\n",
    "    pygame.draw.rect(screen, (200, 200, 200), button_rect)\n",
    "    pygame.draw.rect(screen, (0, 0, 0), button_rect, 2)\n",
    "    screen.blit(button_text, button_rect.topleft)\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0b3225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 159.60718250274658 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size_bytes = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "print(\"Model size:\", model_size_mb, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac92b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "# Quantize the model \n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d707db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 77.961181640625 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the size of the quantized model\n",
    "quantized_model_size_bytes = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad)\n",
    "quantized_model_size_mb = quantized_model_size_bytes / (1024 * 1024)\n",
    "print(\"Quantized model size:\", quantized_model_size_mb, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466e66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
