{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511890a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Face_Emotion_CNN(nn.Module):\n",
    "  def _init_(self):\n",
    "    super(Face_Emotion_CNN, self)._init_()\n",
    "    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n",
    "    self.cnn2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "    self.cnn3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "    self.cnn4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "    self.cnn5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "    self.cnn6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "    self.cnn7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.pool1 = nn.MaxPool2d(2, 1)\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.cnn1_bn = nn.BatchNorm2d(8)\n",
    "    self.cnn2_bn = nn.BatchNorm2d(16)\n",
    "    self.cnn3_bn = nn.BatchNorm2d(32)\n",
    "    self.cnn4_bn = nn.BatchNorm2d(64)\n",
    "    self.cnn5_bn = nn.BatchNorm2d(128)\n",
    "    self.cnn6_bn = nn.BatchNorm2d(256)\n",
    "    self.cnn7_bn = nn.BatchNorm2d(256)\n",
    "    self.fc1 = nn.Linear(1024, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 7)\n",
    "    self.dropout = nn.Dropout(0.3)\n",
    "    self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.pool1(self.cnn1_bn(self.cnn1(x))))\n",
    "    x = self.relu(self.pool1(self.cnn2_bn(self.dropout(self.cnn2(x)))))\n",
    "    x = self.relu(self.pool1(self.cnn3_bn(self.cnn3(x))))\n",
    "    x = self.relu(self.pool1(self.cnn4_bn(self.dropout(self.cnn4(x)))))\n",
    "    x = self.relu(self.pool2(self.cnn5_bn(self.cnn5(x))))\n",
    "    x = self.relu(self.pool2(self.cnn6_bn(self.dropout(self.cnn6(x)))))\n",
    "    x = self.relu(self.pool2(self.cnn7_bn(self.dropout(self.cnn7(x)))))\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    \n",
    "    x = self.relu(self.dropout(self.fc1(x)))\n",
    "    x = self.relu(self.dropout(self.fc2(x)))\n",
    "    x = self.log_softmax(self.fc3(x))\n",
    "    return x\n",
    "\n",
    "  def count_parameters(self):\n",
    "    return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    bn_model = Face_Emotion_CNN()\n",
    "    x = torch.randn(1,1,48,48)\n",
    "    print('Shape of output = ',bn_model(x).shape)\n",
    "    print('No of Parameters of the BatchNorm-CNN Model =',bn_model.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adffb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "# from model import *\n",
    "\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    model = Face_Emotion_CNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage), strict=False)\n",
    "    return model\n",
    "\n",
    "def FER_live_cam():\n",
    "\n",
    "    model = load_trained_model('FER_trained_model.pt')\n",
    "    \n",
    "    emotion_dict = {0: 'neutral', 1: 'happiness'}\n",
    "\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "            resize_frame = cv2.resize(gray[y:y + h, x:x + w], (48, 48))\n",
    "            X = resize_frame/256\n",
    "            X = Image.fromarray((X))\n",
    "            X = val_transform(X).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                log_ps = model.cpu()(X)\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                pred = emotion_dict[int(top_class.numpy())]\n",
    "            cv2.putText(frame, pred, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1)\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "\n",
    "    FER_live_cam()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
